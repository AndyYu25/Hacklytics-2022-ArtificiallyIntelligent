{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## The Deepnote Notebook\nThe most important part of Deepnote is the notebook. This is where the magic happens: it's where you read, analyze, and visualize your data.",
   "metadata": {
    "tags": [],
    "cell_id": "00000-06f75d06-f215-4f8b-ba02-28db85119dca",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "32f25b58-711c-46d4-9fdf-f509615bd89e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "626dc807",
    "execution_start": 1645360688667,
    "execution_millis": 2519,
    "deepnote_cell_type": "code"
   },
   "source": "import os\nimport io\nimport random\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.datasets.folder import default_loader\nfrom torch.utils.data import RandomSampler\nimport torchvision.transforms as tt\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d6fb7dc0-8a11-41c4-9f7d-61ece2293dcc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "521739cb",
    "execution_start": 1645360691191,
    "execution_millis": 24,
    "deepnote_cell_type": "code"
   },
   "source": "\n\n\nDATA_DIR = \"/work/media\"\nsample_dir = '/work/output'\n\nimageSize = 256\nbatchSize = 64\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\nlatentSize = 256\nimageRows = 8\nimageCols = 8\nfeatureSize = 64\n\nimageTransforms = tt.Compose([ tt.Resize(imageSize),\n                               tt.CenterCrop(imageSize),\n                               tt.ToTensor()])\n\n\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\n\nfixedLatent = torch.randn(imageRows * imageCols, latentSize, 1, 1, device=device)\n\ntrainDataset = ImageFolder(DATA_DIR, transform=imageTransforms)\n\ntrainDataLoader = DataLoader(trainDataset, batchSize, shuffle=True, num_workers=2, pin_memory=True)\n\n",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "16c7faae-a103-47bf-b884-a34997882ad2",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "feab6724",
    "execution_start": 1645360691218,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "def getRandomImagePath():\n    \"\"\"\n    pulls random image from dataset\n    \"\"\"\n    randFolder = random.choice(os.listdir(DATA_DIR))\n    randImage = random.choice(os.listdir(DATA_DIR + '/' + randFolder))\n    return DATA_DIR + '/' + randFolder + '/' + randImage",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8927eb2e-9ec3-42de-a994-35d7fabf49ad",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "830ac26d",
    "execution_start": 1645360691237,
    "execution_millis": 657607,
    "deepnote_cell_type": "code"
   },
   "source": "generator = nn.Sequential(\n            # input is latentSize x 1 x 1\n              nn.ConvTranspose2d(latentSize, featureSize * 32, 4, 1, 0, bias=False),\n              nn.BatchNorm2d(featureSize * 32),\n              nn.ReLU(True),\n              # state size. (featureSize*32) x 4 x 4\n              nn.ConvTranspose2d(featureSize * 32, featureSize * 16, 4, 2, 0, bias=False),\n              nn.BatchNorm2d(featureSize * 16),\n              nn.ReLU(True),\n              # state size. (featureSize*16) x 8 x 8\n              nn.ConvTranspose2d(featureSize * 16, featureSize * 8, 4, 2, 1, bias=False),\n              nn.BatchNorm2d(featureSize * 8),\n              nn.ReLU(True),\n              # state size. (featureSize*8) x 16 x 16\n              nn.ConvTranspose2d( featureSize * 8, featureSize * 4, 4, 2, 1, bias=False),\n              nn.BatchNorm2d(featureSize * 4),\n              nn.ReLU(True),\n              # state size. (featureSize*4) x 32 x 32\n              nn.ConvTranspose2d( featureSize * 4, featureSize * 2, 4, 2, 1, bias=False),\n              nn.BatchNorm2d(featureSize * 2),\n              nn.ReLU(True),\n              # state size. (featureSize*2) x 64 x 64\n              nn.ConvTranspose2d( featureSize * 2, featureSize, 4, 2, 1, bias=False),\n              nn.BatchNorm2d(featureSize),\n              nn.ReLU(True),\n              # state size. (featureSize) x 128 x 128\n              nn.ConvTranspose2d(featureSize, 3, 4, 2, 1, bias=False),\n              nn.Tanh()\n              # state size. 3 x 256 x 256\n            )\n\n\n    \n\ndef denorm(img_tensors):\n    return img_tensors * stats[1][0] + stats[0][0]\n\ndef saveSamples(index, latent_tensors, imageRows, imageCols, show=True):\n    fake_images = generator(latent_tensors)\n    fake_fname = sample_dir +'/' + 'generated-images-{0:0=4d}.png'.format(index)\n    print(fake_images.size())\n    save_image(denorm(fake_images), fake_fname, nrow=imageRows)\n\ndef merge(img1, img2, w, h, transparency):\n    \"\"\"\n    Superimpose image 2 on top of image 1 with a transparency filter.\n    Both img1 and img2 need to be in the form of a PIL image.\n    Also resizes both images to specified width and height.\n    Returns a modified img1.\n    \"\"\"\n    # resize the image\n    size = (w,h)\n    img2 = img2.resize(size,Image.ANTIALIAS)\n    img1 = img1.resize(size,Image.ANTIALIAS)\n    im = Image.blend(img2, img1, transparency)\n    return im\n\nfrom google.cloud import vision\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'hackalytics-2022-edddba28c1d0.json'\nclient = vision.ImageAnnotatorClient()\n\ndef faceDiscriminator(image):\n    \"\"\"returns the landmarking confience of Vision as float for the first face detected.\n    Returns 0 if Vision cannot detect a face.\"\"\"\n    with io.open(image, 'rb') as image_file:\n        content = image_file.read()\n    vision_image = vision.Image(content=content)\n\n    response = client.face_detection(image=vision_image)\n    faces = response.face_annotations\n    for face in faces:\n        return face.landmarking_confidence\n    return 0\n\n\n\ndef discriminator(images):\n    \"\"\"\n    Vision determines if a series of images can be\n    landmarked or not.\n    Takes in a tensor of images.\n    Returns a tensor of the shape (images.size(0), 1)\n    \"\"\"\n    outTensor = torch.zeros(images.size(0), 1)\n    images = torch.clone(images).detach()\n    for idx, image in enumerate(images):\n        img = torch.transpose(image, 0, 2)\n        img = img.numpy()\n        img = Image.fromarray(img, mode=\"RGB\")\n        baseImg = Image.open(getRandomImagePath())\n        merge(img, baseImg, imageSize, imageSize, 0.95).save(\"temp.png\")\n        outTensor[idx] = faceDiscriminator(\"temp.png\")\n    return outTensor.requires_grad_()\n\ndef assessDiscriminator():\n    latent = torch.randn(batchSize, latentSize, 1, 1, device=device)\n    fake_images = generator(latent)\n    fake_targets = torch.ones(fake_images.size(0), 1, device=device)\n    fake_preds = discriminator(fake_images)\n    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n    score = torch.mean(fake_preds).item()\n    return score\n    \n\ndef train_generator(opt_g):\n    # Clear generator gradients\n    opt_g.zero_grad()\n    \n    # Generate fake images\n    latent = torch.randn(batchSize, latentSize, 1, 1, device=device)\n    fake_images = generator(latent)\n    \n    # Try to fool the discriminator\n    preds = discriminator(fake_images)\n    targets = torch.ones(batchSize, 1, device=device)\n    loss = F.binary_cross_entropy(preds, targets)\n    \n    # Update generator weights\n    loss.backward()\n    opt_g.step()\n    \n    return loss.item()\n\n\ndef fit(epochs, lr, start_idx=1):\n    torch.cuda.empty_cache()\n    \n    # Losses & scores\n    losses_g = []\n    scores = []\n    \n    # Create optimizers\n    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n    \n    for epoch in range(1, epochs + 1):\n        for real_images, _ in tqdm(trainDataLoader):\n            # Train generator\n            score = assessDiscriminator();\n            loss_g = train_generator(opt_g)\n            \n        # Record losses & scores\n        losses_g.append(loss_g)\n        scores.append(score)\n        \n        # Log losses & scores (last batch)\n        print(\"Epoch [{}/{}], loss_g: {:.4f}, score: {:.4f}\".format(\n            epoch+1, epochs, loss_g, score))\n        # Save model weights\n        generatorWeights = torch.save(generator, sample_dir + '/' + \n                                      str(epoch) + 'generator.pth')    \n\n        # Save generated images\n        saveSamples(epoch+start_idx, fixedLatent, imageRows, imageCols, show=False)\n\n\n    \n    return losses_g, scores\n\n\n\nif __name__ == '__main__':\n    lr = 0.0002\n    epochs = 200\n    history = fit(epochs, lr)\n    losses_g, scores = history",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec59f96e0ad4b8c8ac9d9bee7365275"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d80e1763-e9c4-41f1-9d3f-7a71b565f358' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "b9694295-92fc-4a3f-98b3-f31609abf37a",
  "deepnote_execution_queue": [
   {
    "cellId": "8927eb2e-9ec3-42de-a994-35d7fabf49ad",
    "sessionId": "dfe94ec0-607b-46e7-a5e1-289b766226f2",
    "msgId": "6687255e-7611-48ef-ba41-58d1d09d4ca0"
   }
  ]
 }
}